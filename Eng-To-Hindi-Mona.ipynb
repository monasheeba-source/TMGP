{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü™î Project 3: English to Hindi (Text Translation)\n",
    "\n",
    "üßæ**Description:** The IIT Bombay English-Hindi corpus contains parallel corpus for English-Hindi as well as monolingual Hindi corpus collected from a variety of existing sources and corpora developed at the Center for Indian Language Technology, IIT Bombay over the years. This page describes the corpus. This corpus has been used at the Workshop on Asian Language Translation Shared Task since 2016 the Hindi-to-English and English-to-Hindi languages pairs and as a pivot language pair for the Hindi-to-Japanese and Japanese-to-Hindi language pairs.\n",
    "\n",
    "source of the dataset - https://www.cfilt.iitb.ac.in/iitb_parallel/\n",
    "\n",
    "research paper - The IIT Bombay English-Hindi Parallel Corpus  - https://arxiv.org/pdf/1710.02855.pdf\n",
    "\n",
    "üß≠ **Problem Statement:** You are provided with a large dataset of language pairs, parallelly in English and Hindi: you have to perform a step-by-step NLP approach to translate English to Hindi after splitting the dataset into train-test-validation sets.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\monas\\AppData\\Local\\Temp\\ipykernel_10376\\4207489347.py:29: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "import string\n",
    "from string import digits\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "#print(os.listdir(\"../input\"))\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"hindi_english_parallel.csv\",encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hindi</th>\n",
       "      <th>english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>‡§Ö‡§™‡§®‡•á ‡§Ö‡§®‡•Å‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó ‡§ï‡•ã ‡§™‡§π‡•Å‡§Ç‡§ö‡§®‡•Ä‡§Ø‡§§‡§æ ‡§µ‡•ç‡§Ø‡§æ‡§Ø‡§æ‡§Æ ‡§ï‡§æ ‡§≤‡§æ‡§≠ ‡§¶‡•á‡§Ç</td>\n",
       "      <td>Give your application an accessibility workout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‡§è‡§ï‡•ç‡§∏‡•á‡§∞‡•ç‡§∏‡§æ‡§á‡§∏‡§∞ ‡§™‡§π‡•Å‡§Ç‡§ö‡§®‡•Ä‡§Ø‡§§‡§æ ‡§Ö‡§®‡•ç‡§µ‡•á‡§∑‡§ï</td>\n",
       "      <td>Accerciser Accessibility Explorer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>‡§®‡§ø‡§ö‡§≤‡•á ‡§™‡§ü‡§≤ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§°‡§ø‡§´‡•ã‡§≤‡•ç‡§ü ‡§™‡•ç‡§≤‡§ó-‡§á‡§® ‡§ñ‡§æ‡§ï‡§æ</td>\n",
       "      <td>The default plugin layout for the bottom panel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>‡§ä‡§™‡§∞‡•Ä ‡§™‡§ü‡§≤ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§°‡§ø‡§´‡•ã‡§≤‡•ç‡§ü ‡§™‡•ç‡§≤‡§ó-‡§á‡§® ‡§ñ‡§æ‡§ï‡§æ</td>\n",
       "      <td>The default plugin layout for the top panel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>‡§â‡§® ‡§™‡•ç‡§≤‡§ó-‡§á‡§®‡•ã‡§Ç ‡§ï‡•Ä ‡§∏‡•Ç‡§ö‡•Ä ‡§ú‡§ø‡§®‡•ç‡§π‡•á‡§Ç ‡§°‡§ø‡§´‡•ã‡§≤‡•ç‡§ü ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§®‡§ø‡§∑‡•ç‡§ï‡•ç‡§∞‡§ø‡§Ø ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•à</td>\n",
       "      <td>A list of plugins that are disabled by default</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               hindi                                         english\n",
       "0  ‡§Ö‡§™‡§®‡•á ‡§Ö‡§®‡•Å‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó ‡§ï‡•ã ‡§™‡§π‡•Å‡§Ç‡§ö‡§®‡•Ä‡§Ø‡§§‡§æ ‡§µ‡•ç‡§Ø‡§æ‡§Ø‡§æ‡§Æ ‡§ï‡§æ ‡§≤‡§æ‡§≠ ‡§¶‡•á‡§Ç                    Give your application an accessibility workout\n",
       "1  ‡§è‡§ï‡•ç‡§∏‡•á‡§∞‡•ç‡§∏‡§æ‡§á‡§∏‡§∞ ‡§™‡§π‡•Å‡§Ç‡§ö‡§®‡•Ä‡§Ø‡§§‡§æ ‡§Ö‡§®‡•ç‡§µ‡•á‡§∑‡§ï                                    Accerciser Accessibility Explorer             \n",
       "2  ‡§®‡§ø‡§ö‡§≤‡•á ‡§™‡§ü‡§≤ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§°‡§ø‡§´‡•ã‡§≤‡•ç‡§ü ‡§™‡•ç‡§≤‡§ó-‡§á‡§® ‡§ñ‡§æ‡§ï‡§æ                              The default plugin layout for the bottom panel\n",
       "3  ‡§ä‡§™‡§∞‡•Ä ‡§™‡§ü‡§≤ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§°‡§ø‡§´‡•ã‡§≤‡•ç‡§ü ‡§™‡•ç‡§≤‡§ó-‡§á‡§® ‡§ñ‡§æ‡§ï‡§æ                               The default plugin layout for the top panel   \n",
       "4  ‡§â‡§® ‡§™‡•ç‡§≤‡§ó-‡§á‡§®‡•ã‡§Ç ‡§ï‡•Ä ‡§∏‡•Ç‡§ö‡•Ä ‡§ú‡§ø‡§®‡•ç‡§π‡•á‡§Ç ‡§°‡§ø‡§´‡•ã‡§≤‡•ç‡§ü ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§®‡§ø‡§∑‡•ç‡§ï‡•ç‡§∞‡§ø‡§Ø ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•à  A list of plugins that are disabled by default"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1561841, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1561841 entries, 0 to 1561840\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count    Dtype \n",
      "---  ------   --------------    ----- \n",
      " 0   hindi    1555785 non-null  object\n",
      " 1   english  1561116 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 23.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hindi      0.39\n",
       "english    0.05\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for NULL values\n",
    "round((df.isnull().sum()/len(df['hindi']))*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since 40% of data in hindi have no translation, it is of no use to have it and we have to drop them\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hindi      0\n",
       "english    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1555727, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1353912, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df['hindi'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df=df[df['source']=='ted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hindi</th>\n",
       "      <th>english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>‡§Ö‡§™‡§®‡•á ‡§Ö‡§®‡•Å‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó ‡§ï‡•ã ‡§™‡§π‡•Å‡§Ç‡§ö‡§®‡•Ä‡§Ø‡§§‡§æ ‡§µ‡•ç‡§Ø‡§æ‡§Ø‡§æ‡§Æ ‡§ï‡§æ ‡§≤‡§æ‡§≠ ‡§¶‡•á‡§Ç</td>\n",
       "      <td>Give your application an accessibility workout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‡§è‡§ï‡•ç‡§∏‡•á‡§∞‡•ç‡§∏‡§æ‡§á‡§∏‡§∞ ‡§™‡§π‡•Å‡§Ç‡§ö‡§®‡•Ä‡§Ø‡§§‡§æ ‡§Ö‡§®‡•ç‡§µ‡•á‡§∑‡§ï</td>\n",
       "      <td>Accerciser Accessibility Explorer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>‡§®‡§ø‡§ö‡§≤‡•á ‡§™‡§ü‡§≤ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§°‡§ø‡§´‡•ã‡§≤‡•ç‡§ü ‡§™‡•ç‡§≤‡§ó-‡§á‡§® ‡§ñ‡§æ‡§ï‡§æ</td>\n",
       "      <td>The default plugin layout for the bottom panel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>‡§ä‡§™‡§∞‡•Ä ‡§™‡§ü‡§≤ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§°‡§ø‡§´‡•ã‡§≤‡•ç‡§ü ‡§™‡•ç‡§≤‡§ó-‡§á‡§® ‡§ñ‡§æ‡§ï‡§æ</td>\n",
       "      <td>The default plugin layout for the top panel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>‡§â‡§® ‡§™‡•ç‡§≤‡§ó-‡§á‡§®‡•ã‡§Ç ‡§ï‡•Ä ‡§∏‡•Ç‡§ö‡•Ä ‡§ú‡§ø‡§®‡•ç‡§π‡•á‡§Ç ‡§°‡§ø‡§´‡•ã‡§≤‡•ç‡§ü ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§®‡§ø‡§∑‡•ç‡§ï‡•ç‡§∞‡§ø‡§Ø ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•à</td>\n",
       "      <td>A list of plugins that are disabled by default</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>‡§Ö‡§µ‡§ß‡§ø ‡§ï‡•ã ‡§π‡§æ‡§á‡§≤‡§æ‡§á‡§ü ‡§∞‡§ï‡•á‡§Ç</td>\n",
       "      <td>Highlight duration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>‡§™‡§π‡•Å‡§Ç‡§ö‡§®‡•Ä‡§Ø ‡§Ü‡§∏‡§Ç‡§ß‡§ø (‡§®‡•ã‡§°) ‡§ï‡•ã ‡§ö‡•Å‡§®‡§§‡•á ‡§∏‡§Æ‡§Ø ‡§π‡§æ‡§á‡§≤‡§æ‡§á‡§ü ‡§¨‡§ï‡•ç‡§∏‡•á ‡§ï‡•Ä ‡§Ö‡§µ‡§ß‡§ø</td>\n",
       "      <td>The duration of the highlight box when selecting accessible nodes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>‡§∏‡•Ä‡§Æ‡§æ‡§Ç‡§§ (‡§¨‡•ã‡§∞‡•ç‡§°‡§∞) ‡§ï‡•á ‡§∞‡§Ç‡§ó ‡§ï‡•ã ‡§π‡§æ‡§á‡§≤‡§æ‡§á‡§ü ‡§ï‡§∞‡•á‡§Ç</td>\n",
       "      <td>Highlight border color</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>‡§π‡§æ‡§á‡§≤‡§æ‡§á‡§ü ‡§ï‡§ø‡§è ‡§ó‡§è ‡§∏‡•Ä‡§Æ‡§æ‡§Ç‡§§ ‡§ï‡§æ ‡§∞‡§Ç‡§ó ‡§î‡§∞ ‡§Ö‡§™‡§æ‡§∞‡§¶‡§∞‡•ç‡§∂‡§ø‡§§‡§æ‡•§</td>\n",
       "      <td>The color and opacity of the highlight border.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>‡§≠‡§∞‡§æ‡§à ‡§ï‡•á ‡§∞‡§Ç‡§ó ‡§ï‡•ã ‡§π‡§æ‡§á‡§≤‡§æ‡§á‡§ü ‡§ï‡§∞‡•á‡§Ç</td>\n",
       "      <td>Highlight fill color</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>‡§π‡§æ‡§á‡§≤‡§æ‡§á‡§ü ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§≠‡§∞‡§æ‡§à ‡§ï‡§æ ‡§∞‡§Ç‡§ó ‡§î‡§∞ ‡§™‡§æ‡§∞‡§¶‡§∞‡•ç‡§∂‡§ø‡§§‡§æ‡•§</td>\n",
       "      <td>The color and opacity of the highlight fill.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>‡§è‡§™‡•Ä‡§Ü‡§à ‡§µ‡§ø‡§ö‡§∞‡§ï</td>\n",
       "      <td>API Browser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>‡§á‡§∏ ‡§∏‡§Æ‡§Ø ‡§ú‡§ø‡§∏‡•á ‡§™‡•ç‡§∞‡§æ‡§™‡•ç‡§§ ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•ã, ‡§â‡§∏‡§ï‡•Ä ‡§µ‡§ø‡§≠‡§ø‡§®‡•ç‡§® ‡§µ‡§ø‡§ß‡§ø‡§Ø‡•ã‡§Ç (‡§Æ‡•á‡§•‡§°) ‡§Æ‡•á‡§Ç ‡§µ‡§ø‡§ö‡§∞‡§£ ‡§ï‡§∞‡•á‡§Ç</td>\n",
       "      <td>Browse the various methods of the current accessible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>‡§®‡§ø‡§ú‡•Ä ‡§ó‡•Å‡§£‡•ã‡§Ç ‡§ï‡•ã ‡§õ‡§ø‡§™‡§æ‡§è‡§Ç</td>\n",
       "      <td>Hide private attributes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>‡§µ‡§ø‡§ß‡§ø</td>\n",
       "      <td>Method</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>‡§ó‡•Å‡§£‡§ß‡§∞‡•ç‡§Æ</td>\n",
       "      <td>Property</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>‡§Æ‡§æ‡§®</td>\n",
       "      <td>Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>‡§Ü‡§à‡§™‡§æ‡§á‡§•‡§® ‡§ï‡§®‡•ç‡§∏‡•ã‡§≤</td>\n",
       "      <td>IPython Console</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>‡§á‡§∏ ‡§∏‡§Æ‡§Ø ‡§ö‡•Å‡§®‡•á ‡§ó‡§è ‡§è‡§ï‡•ç‡§∏‡•á‡§∏‡•á‡§¨‡•á‡§≤ ‡§∏‡•á ‡§ï‡§æ‡§Æ ‡§≤‡•á‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ö‡§Ç‡§§‡§∞‡•ç‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ‡§§‡•ç‡§Æ‡§ï ‡§ï‡§®‡•ç‡§∏‡•ã‡§≤</td>\n",
       "      <td>Interactive console for manipulating currently selected accessible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>‡§ò‡§ü‡§®‡§æ ‡§Æ‡§æ‡§®‡§ø‡§ü‡§∞</td>\n",
       "      <td>Event monitor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                          hindi                                                             english\n",
       "0   ‡§Ö‡§™‡§®‡•á ‡§Ö‡§®‡•Å‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó ‡§ï‡•ã ‡§™‡§π‡•Å‡§Ç‡§ö‡§®‡•Ä‡§Ø‡§§‡§æ ‡§µ‡•ç‡§Ø‡§æ‡§Ø‡§æ‡§Æ ‡§ï‡§æ ‡§≤‡§æ‡§≠ ‡§¶‡•á‡§Ç                              Give your application an accessibility workout                    \n",
       "1   ‡§è‡§ï‡•ç‡§∏‡•á‡§∞‡•ç‡§∏‡§æ‡§á‡§∏‡§∞ ‡§™‡§π‡•Å‡§Ç‡§ö‡§®‡•Ä‡§Ø‡§§‡§æ ‡§Ö‡§®‡•ç‡§µ‡•á‡§∑‡§ï                                              Accerciser Accessibility Explorer                                 \n",
       "2   ‡§®‡§ø‡§ö‡§≤‡•á ‡§™‡§ü‡§≤ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§°‡§ø‡§´‡•ã‡§≤‡•ç‡§ü ‡§™‡•ç‡§≤‡§ó-‡§á‡§® ‡§ñ‡§æ‡§ï‡§æ                                        The default plugin layout for the bottom panel                    \n",
       "3   ‡§ä‡§™‡§∞‡•Ä ‡§™‡§ü‡§≤ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§°‡§ø‡§´‡•ã‡§≤‡•ç‡§ü ‡§™‡•ç‡§≤‡§ó-‡§á‡§® ‡§ñ‡§æ‡§ï‡§æ                                         The default plugin layout for the top panel                       \n",
       "4   ‡§â‡§® ‡§™‡•ç‡§≤‡§ó-‡§á‡§®‡•ã‡§Ç ‡§ï‡•Ä ‡§∏‡•Ç‡§ö‡•Ä ‡§ú‡§ø‡§®‡•ç‡§π‡•á‡§Ç ‡§°‡§ø‡§´‡•ã‡§≤‡•ç‡§ü ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§®‡§ø‡§∑‡•ç‡§ï‡•ç‡§∞‡§ø‡§Ø ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•à            A list of plugins that are disabled by default                    \n",
       "5   ‡§Ö‡§µ‡§ß‡§ø ‡§ï‡•ã ‡§π‡§æ‡§á‡§≤‡§æ‡§á‡§ü ‡§∞‡§ï‡•á‡§Ç                                                         Highlight duration                                                \n",
       "6   ‡§™‡§π‡•Å‡§Ç‡§ö‡§®‡•Ä‡§Ø ‡§Ü‡§∏‡§Ç‡§ß‡§ø (‡§®‡•ã‡§°) ‡§ï‡•ã ‡§ö‡•Å‡§®‡§§‡•á ‡§∏‡§Æ‡§Ø ‡§π‡§æ‡§á‡§≤‡§æ‡§á‡§ü ‡§¨‡§ï‡•ç‡§∏‡•á ‡§ï‡•Ä ‡§Ö‡§µ‡§ß‡§ø                      The duration of the highlight box when selecting accessible nodes \n",
       "7   ‡§∏‡•Ä‡§Æ‡§æ‡§Ç‡§§ (‡§¨‡•ã‡§∞‡•ç‡§°‡§∞) ‡§ï‡•á ‡§∞‡§Ç‡§ó ‡§ï‡•ã ‡§π‡§æ‡§á‡§≤‡§æ‡§á‡§ü ‡§ï‡§∞‡•á‡§Ç                                       Highlight border color                                            \n",
       "8   ‡§π‡§æ‡§á‡§≤‡§æ‡§á‡§ü ‡§ï‡§ø‡§è ‡§ó‡§è ‡§∏‡•Ä‡§Æ‡§æ‡§Ç‡§§ ‡§ï‡§æ ‡§∞‡§Ç‡§ó ‡§î‡§∞ ‡§Ö‡§™‡§æ‡§∞‡§¶‡§∞‡•ç‡§∂‡§ø‡§§‡§æ‡•§                                 The color and opacity of the highlight border.                    \n",
       "9   ‡§≠‡§∞‡§æ‡§à ‡§ï‡•á ‡§∞‡§Ç‡§ó ‡§ï‡•ã ‡§π‡§æ‡§á‡§≤‡§æ‡§á‡§ü ‡§ï‡§∞‡•á‡§Ç                                                  Highlight fill color                                              \n",
       "10  ‡§π‡§æ‡§á‡§≤‡§æ‡§á‡§ü ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§≠‡§∞‡§æ‡§à ‡§ï‡§æ ‡§∞‡§Ç‡§ó ‡§î‡§∞ ‡§™‡§æ‡§∞‡§¶‡§∞‡•ç‡§∂‡§ø‡§§‡§æ‡•§                                  The color and opacity of the highlight fill.                      \n",
       "11  ‡§è‡§™‡•Ä‡§Ü‡§à ‡§µ‡§ø‡§ö‡§∞‡§ï                                                                  API Browser                                                       \n",
       "12  ‡§á‡§∏ ‡§∏‡§Æ‡§Ø ‡§ú‡§ø‡§∏‡•á ‡§™‡•ç‡§∞‡§æ‡§™‡•ç‡§§ ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•ã, ‡§â‡§∏‡§ï‡•Ä ‡§µ‡§ø‡§≠‡§ø‡§®‡•ç‡§® ‡§µ‡§ø‡§ß‡§ø‡§Ø‡•ã‡§Ç (‡§Æ‡•á‡§•‡§°) ‡§Æ‡•á‡§Ç ‡§µ‡§ø‡§ö‡§∞‡§£ ‡§ï‡§∞‡•á‡§Ç  Browse the various methods of the current accessible              \n",
       "13  ‡§®‡§ø‡§ú‡•Ä ‡§ó‡•Å‡§£‡•ã‡§Ç ‡§ï‡•ã ‡§õ‡§ø‡§™‡§æ‡§è‡§Ç                                                         Hide private attributes                                           \n",
       "14  ‡§µ‡§ø‡§ß‡§ø                                                                         Method                                                            \n",
       "15  ‡§ó‡•Å‡§£‡§ß‡§∞‡•ç‡§Æ                                                                      Property                                                          \n",
       "16  ‡§Æ‡§æ‡§®                                                                          Value                                                             \n",
       "17  ‡§Ü‡§à‡§™‡§æ‡§á‡§•‡§® ‡§ï‡§®‡•ç‡§∏‡•ã‡§≤                                                               IPython Console                                                   \n",
       "18  ‡§á‡§∏ ‡§∏‡§Æ‡§Ø ‡§ö‡•Å‡§®‡•á ‡§ó‡§è ‡§è‡§ï‡•ç‡§∏‡•á‡§∏‡•á‡§¨‡•á‡§≤ ‡§∏‡•á ‡§ï‡§æ‡§Æ ‡§≤‡•á‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ö‡§Ç‡§§‡§∞‡•ç‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ‡§§‡•ç‡§Æ‡§ï ‡§ï‡§®‡•ç‡§∏‡•ã‡§≤          Interactive console for manipulating currently selected accessible\n",
       "19  ‡§ò‡§ü‡§®‡§æ ‡§Æ‡§æ‡§®‡§ø‡§ü‡§∞                                                                  Event monitor                                                     "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hindi      0\n",
       "english    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.isnull(df).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[~pd.isnull(df['english'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Let us pick any 25000 rows from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.sample(n=25000,random_state=42)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase all characters\n",
    "df['english']=df['english'].apply(lambda x: x.lower())\n",
    "df['hindi']=df['hindi'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove quotes\n",
    "df['english']=df['english'].apply(lambda x: re.sub(\"'\", '', x))\n",
    "df['hindi']=df['hindi'].apply(lambda x: re.sub(\"'\", '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = set(string.punctuation) # Set of all special characters\n",
    "# Remove all the special characters\n",
    "df['english']=df['english'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "df['hindi']=df['hindi'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all numbers from text\n",
    "remove_digits = str.maketrans('', '', digits)\n",
    "df['english']=df['english'].apply(lambda x: x.translate(remove_digits))\n",
    "df['hindi']=df['hindi'].apply(lambda x: x.translate(remove_digits))\n",
    "\n",
    "df['hindi'] = df['hindi'].apply(lambda x: re.sub(\"[‡•®‡•©‡•¶‡•Æ‡•ß‡•´‡•≠‡•Ø‡•™‡•¨]\", \"\", x))\n",
    "\n",
    "# Remove extra spaces\n",
    "df['english']=df['english'].apply(lambda x: x.strip())\n",
    "df['hindi']=df['hindi'].apply(lambda x: x.strip())\n",
    "df['english']=df['english'].apply(lambda x: re.sub(\" +\", \" \", x))\n",
    "df['hindi']=df['hindi'].apply(lambda x: re.sub(\" +\", \" \", x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add start and end tokens to target sequences\n",
    "df['hindi'] = df['hindi'].apply(lambda x : 'START_ '+ x + ' _END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hindi</th>\n",
       "      <th>english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1255665</th>\n",
       "      <td>START_ ‡§Æ‡§π‡§æ‡§∞‡§æ‡§ú‡§æ ‡§ï‡•â‡§≤‡§ø‡§ú ‡§ú‡§Ø‡§™‡•Å‡§∞ _END</td>\n",
       "      <td>maharaja college jaipur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238476</th>\n",
       "      <td>START_ ‡§á‡§® ‡§ó‡•â‡§° ‡§µ‡•Ä ‡§ü‡•ç‡§∞‡§∏‡•ç‡§ü _END</td>\n",
       "      <td>in god we trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1411628</th>\n",
       "      <td>START_ ‡§µ‡§π ‡§®‡•å‡§ï‡§∞‡•Ä ‡§¢‡•Ç‡§Å‡§¢‡§º‡§®‡•á ‡§Æ‡•á‡§Ç ‡§Ö‡§™‡§®‡•Ä ‡§Ö‡§∏‡§´‡§≤‡§§‡§æ ‡§ï‡•Ä ‡§ö‡§∞‡•ç‡§ö‡§æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§Ö‡§Ç‡§§ ‡§Æ‡•á‡§Ç ‡§µ‡§π ‡§® ‡§™‡§æ‡§∏ ‡§® ‡§´‡§º‡•Ä‡§∏ ‡§ï‡•á ‡§¨‡§æ‡§¶ ‡§™‡§∞ ‡§ö‡§≤‡§æ‡§Ø‡•Ä ‡§ú‡§æ ‡§∞‡§π‡•Ä ‡§è‡§ï ‡§ï‡§ï‡•ç‡§∑‡§æ ‡§Æ‡•á‡§Ç ‡§µ‡§ø‡§¶‡•ç‡§Ø‡§æ‡§∞‡•ç‡§•‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•ã ‡§™‡§æ‡§† ‡§∞‡§ü‡§æ ‡§∞‡§π‡§æ ‡§π‡•ã‡§§‡§æ ‡§π‡•à‡•§ _END</td>\n",
       "      <td>he recounts his unsuccessful attempts to get a job he is finally left running a crammer s class with the promise no pass no fees</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547009</th>\n",
       "      <td>START_ ‡§∞‡§æ‡§ú‡§∂‡§æ‡§π‡•Ä ‡§∏‡§æ‡§Æ‡•ç‡§∞‡§æ‡§ú‡•ç‡§Ø ‡§¨‡§π‡§æ‡§¶‡•Å‡§∞ ‡§∂‡§æ‡§π ‡§¶‡•ç‡§µ‡§ø‡§§‡•Ä‡§Ø ‡§ï‡•á ‡§¨‡§æ‡§¶ ‡§∏‡§Æ‡§æ‡§™‡•ç‡§§ ‡§π‡•ã ‡§ó‡§Ø‡§æ ‡§ú‡•ã ‡§∏‡§ø‡§™‡§æ‡§π‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•Ä ‡§¨‡§ó‡§æ‡§µ‡§§ ‡§Æ‡•á‡§Ç ‡§∏‡§π‡§æ‡§Ø‡§§‡§æ ‡§¶‡•á‡§®‡•á ‡§ï‡•á ‡§∏‡§Ç‡§¶‡•á‡§π ‡§™‡§∞ ‡§¨‡•ç‡§∞‡§ø‡§ü‡§ø‡§∂ ‡§∞‡§æ‡§ú ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§∞‡§Ç‡§ó‡•Ç‡§® ‡§®‡§ø‡§∞‡•ç‡§µ‡§æ‡§∏‡§ø‡§§ ‡§ï‡§∞ ‡§¶‡§ø‡§è ‡§ó‡§è ‡§•‡•á‡•§ ‡§µ‡§π‡§æ‡§Ç ‡§Æ‡•á‡§Ç ‡§â‡§®‡§ï‡•Ä ‡§Æ‡•É‡§§‡•ç‡§Ø‡•Å ‡§π‡•ã ‡§ó‡§à‡•§ _END</td>\n",
       "      <td>the imperial dynasty became extinct with bahadur shah ii who was deported to rangoon by the british on suspicion of assisting the sepoy mutineers he died there in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1344715</th>\n",
       "      <td>START_ ‡§∏‡§Ç‡§Ø‡•Å‡§ï‡•ç‡§§ ‡§∞‡§æ‡§ú‡•ç‡§Ø ‡§Ö‡§Æ‡•á‡§∞‡§ø‡§ï‡§æ ‡§ï‡•á ‡§™‡•Ç‡§∞‡•ç‡§µ‡•ã‡§§‡•ç‡§§‡§∞ ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞ ‡§Æ‡•á‡§Ç ‡§Ü‡§è ‡§Ü‡§Ç‡§ß‡•Ä‡§§‡•Ç‡§´‡§æ‡§® ‡§Æ‡•á‡§Ç ‡§∏‡§¨‡§∏‡•á ‡§ï‡§Æ ‡§™‡•Ä ‡§è‡§ö ‡§®‡§µ‡§Æ‡•ç‡§¨‡§∞ ‡§Æ‡•á‡§Ç ‡§Æ‡§æ‡§™‡§æ ‡§ó‡§Ø‡§æ ‡§•‡§æ‡•§ _END</td>\n",
       "      <td>the lowest ph value recorded for a storm in northeastern united states was during november</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                        hindi                                                                                                                                                             english\n",
       "1255665  START_ ‡§Æ‡§π‡§æ‡§∞‡§æ‡§ú‡§æ ‡§ï‡•â‡§≤‡§ø‡§ú ‡§ú‡§Ø‡§™‡•Å‡§∞ _END                                                                                                                                                                       maharaja college jaipur                                                                                                                                           \n",
       "1238476  START_ ‡§á‡§® ‡§ó‡•â‡§° ‡§µ‡•Ä ‡§ü‡•ç‡§∞‡§∏‡•ç‡§ü _END                                                                                                                                                                          in god we trust                                                                                                                                                   \n",
       "1411628  START_ ‡§µ‡§π ‡§®‡•å‡§ï‡§∞‡•Ä ‡§¢‡•Ç‡§Å‡§¢‡§º‡§®‡•á ‡§Æ‡•á‡§Ç ‡§Ö‡§™‡§®‡•Ä ‡§Ö‡§∏‡§´‡§≤‡§§‡§æ ‡§ï‡•Ä ‡§ö‡§∞‡•ç‡§ö‡§æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§Ö‡§Ç‡§§ ‡§Æ‡•á‡§Ç ‡§µ‡§π ‡§® ‡§™‡§æ‡§∏ ‡§® ‡§´‡§º‡•Ä‡§∏ ‡§ï‡•á ‡§¨‡§æ‡§¶ ‡§™‡§∞ ‡§ö‡§≤‡§æ‡§Ø‡•Ä ‡§ú‡§æ ‡§∞‡§π‡•Ä ‡§è‡§ï ‡§ï‡§ï‡•ç‡§∑‡§æ ‡§Æ‡•á‡§Ç ‡§µ‡§ø‡§¶‡•ç‡§Ø‡§æ‡§∞‡•ç‡§•‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•ã ‡§™‡§æ‡§† ‡§∞‡§ü‡§æ ‡§∞‡§π‡§æ ‡§π‡•ã‡§§‡§æ ‡§π‡•à‡•§ _END                                       he recounts his unsuccessful attempts to get a job he is finally left running a crammer s class with the promise no pass no fees                                  \n",
       "1547009  START_ ‡§∞‡§æ‡§ú‡§∂‡§æ‡§π‡•Ä ‡§∏‡§æ‡§Æ‡•ç‡§∞‡§æ‡§ú‡•ç‡§Ø ‡§¨‡§π‡§æ‡§¶‡•Å‡§∞ ‡§∂‡§æ‡§π ‡§¶‡•ç‡§µ‡§ø‡§§‡•Ä‡§Ø ‡§ï‡•á ‡§¨‡§æ‡§¶ ‡§∏‡§Æ‡§æ‡§™‡•ç‡§§ ‡§π‡•ã ‡§ó‡§Ø‡§æ ‡§ú‡•ã ‡§∏‡§ø‡§™‡§æ‡§π‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•Ä ‡§¨‡§ó‡§æ‡§µ‡§§ ‡§Æ‡•á‡§Ç ‡§∏‡§π‡§æ‡§Ø‡§§‡§æ ‡§¶‡•á‡§®‡•á ‡§ï‡•á ‡§∏‡§Ç‡§¶‡•á‡§π ‡§™‡§∞ ‡§¨‡•ç‡§∞‡§ø‡§ü‡§ø‡§∂ ‡§∞‡§æ‡§ú ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§∞‡§Ç‡§ó‡•Ç‡§® ‡§®‡§ø‡§∞‡•ç‡§µ‡§æ‡§∏‡§ø‡§§ ‡§ï‡§∞ ‡§¶‡§ø‡§è ‡§ó‡§è ‡§•‡•á‡•§ ‡§µ‡§π‡§æ‡§Ç ‡§Æ‡•á‡§Ç ‡§â‡§®‡§ï‡•Ä ‡§Æ‡•É‡§§‡•ç‡§Ø‡•Å ‡§π‡•ã ‡§ó‡§à‡•§ _END  the imperial dynasty became extinct with bahadur shah ii who was deported to rangoon by the british on suspicion of assisting the sepoy mutineers he died there in\n",
       "1344715  START_ ‡§∏‡§Ç‡§Ø‡•Å‡§ï‡•ç‡§§ ‡§∞‡§æ‡§ú‡•ç‡§Ø ‡§Ö‡§Æ‡•á‡§∞‡§ø‡§ï‡§æ ‡§ï‡•á ‡§™‡•Ç‡§∞‡•ç‡§µ‡•ã‡§§‡•ç‡§§‡§∞ ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞ ‡§Æ‡•á‡§Ç ‡§Ü‡§è ‡§Ü‡§Ç‡§ß‡•Ä‡§§‡•Ç‡§´‡§æ‡§® ‡§Æ‡•á‡§Ç ‡§∏‡§¨‡§∏‡•á ‡§ï‡§Æ ‡§™‡•Ä ‡§è‡§ö ‡§®‡§µ‡§Æ‡•ç‡§¨‡§∞ ‡§Æ‡•á‡§Ç ‡§Æ‡§æ‡§™‡§æ ‡§ó‡§Ø‡§æ ‡§•‡§æ‡•§ _END                                                                                    the lowest ph value recorded for a storm in northeastern united states was during november                                                                        "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get English and Hindi Vocabulary\n",
    "all_eng_words=set()\n",
    "for eng in df['english']:\n",
    "    for word in eng.split():\n",
    "        if word not in all_eng_words:\n",
    "            all_eng_words.add(word)\n",
    "\n",
    "all_hindi_words=set()\n",
    "for hin in df['hindi']:\n",
    "    for word in hin.split():\n",
    "        if word not in all_hindi_words:\n",
    "            all_hindi_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28326"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_eng_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38586"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_hindi_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['length_eng_sentence']=df['english'].apply(lambda x:len(x.split(\" \")))\n",
    "df['length_hin_sentence']=df['hindi'].apply(lambda x:len(x.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hindi</th>\n",
       "      <th>english</th>\n",
       "      <th>length_eng_sentence</th>\n",
       "      <th>length_hin_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1255665</th>\n",
       "      <td>START_ ‡§Æ‡§π‡§æ‡§∞‡§æ‡§ú‡§æ ‡§ï‡•â‡§≤‡§ø‡§ú ‡§ú‡§Ø‡§™‡•Å‡§∞ _END</td>\n",
       "      <td>maharaja college jaipur</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238476</th>\n",
       "      <td>START_ ‡§á‡§® ‡§ó‡•â‡§° ‡§µ‡•Ä ‡§ü‡•ç‡§∞‡§∏‡•ç‡§ü _END</td>\n",
       "      <td>in god we trust</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1411628</th>\n",
       "      <td>START_ ‡§µ‡§π ‡§®‡•å‡§ï‡§∞‡•Ä ‡§¢‡•Ç‡§Å‡§¢‡§º‡§®‡•á ‡§Æ‡•á‡§Ç ‡§Ö‡§™‡§®‡•Ä ‡§Ö‡§∏‡§´‡§≤‡§§‡§æ ‡§ï‡•Ä ‡§ö‡§∞‡•ç‡§ö‡§æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§Ö‡§Ç‡§§ ‡§Æ‡•á‡§Ç ‡§µ‡§π ‡§® ‡§™‡§æ‡§∏ ‡§® ‡§´‡§º‡•Ä‡§∏ ‡§ï‡•á ‡§¨‡§æ‡§¶ ‡§™‡§∞ ‡§ö‡§≤‡§æ‡§Ø‡•Ä ‡§ú‡§æ ‡§∞‡§π‡•Ä ‡§è‡§ï ‡§ï‡§ï‡•ç‡§∑‡§æ ‡§Æ‡•á‡§Ç ‡§µ‡§ø‡§¶‡•ç‡§Ø‡§æ‡§∞‡•ç‡§•‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•ã ‡§™‡§æ‡§† ‡§∞‡§ü‡§æ ‡§∞‡§π‡§æ ‡§π‡•ã‡§§‡§æ ‡§π‡•à‡•§ _END</td>\n",
       "      <td>he recounts his unsuccessful attempts to get a job he is finally left running a crammer s class with the promise no pass no fees</td>\n",
       "      <td>25</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547009</th>\n",
       "      <td>START_ ‡§∞‡§æ‡§ú‡§∂‡§æ‡§π‡•Ä ‡§∏‡§æ‡§Æ‡•ç‡§∞‡§æ‡§ú‡•ç‡§Ø ‡§¨‡§π‡§æ‡§¶‡•Å‡§∞ ‡§∂‡§æ‡§π ‡§¶‡•ç‡§µ‡§ø‡§§‡•Ä‡§Ø ‡§ï‡•á ‡§¨‡§æ‡§¶ ‡§∏‡§Æ‡§æ‡§™‡•ç‡§§ ‡§π‡•ã ‡§ó‡§Ø‡§æ ‡§ú‡•ã ‡§∏‡§ø‡§™‡§æ‡§π‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•Ä ‡§¨‡§ó‡§æ‡§µ‡§§ ‡§Æ‡•á‡§Ç ‡§∏‡§π‡§æ‡§Ø‡§§‡§æ ‡§¶‡•á‡§®‡•á ‡§ï‡•á ‡§∏‡§Ç‡§¶‡•á‡§π ‡§™‡§∞ ‡§¨‡•ç‡§∞‡§ø‡§ü‡§ø‡§∂ ‡§∞‡§æ‡§ú ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§∞‡§Ç‡§ó‡•Ç‡§® ‡§®‡§ø‡§∞‡•ç‡§µ‡§æ‡§∏‡§ø‡§§ ‡§ï‡§∞ ‡§¶‡§ø‡§è ‡§ó‡§è ‡§•‡•á‡•§ ‡§µ‡§π‡§æ‡§Ç ‡§Æ‡•á‡§Ç ‡§â‡§®‡§ï‡•Ä ‡§Æ‡•É‡§§‡•ç‡§Ø‡•Å ‡§π‡•ã ‡§ó‡§à‡•§ _END</td>\n",
       "      <td>the imperial dynasty became extinct with bahadur shah ii who was deported to rangoon by the british on suspicion of assisting the sepoy mutineers he died there in</td>\n",
       "      <td>28</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1344715</th>\n",
       "      <td>START_ ‡§∏‡§Ç‡§Ø‡•Å‡§ï‡•ç‡§§ ‡§∞‡§æ‡§ú‡•ç‡§Ø ‡§Ö‡§Æ‡•á‡§∞‡§ø‡§ï‡§æ ‡§ï‡•á ‡§™‡•Ç‡§∞‡•ç‡§µ‡•ã‡§§‡•ç‡§§‡§∞ ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞ ‡§Æ‡•á‡§Ç ‡§Ü‡§è ‡§Ü‡§Ç‡§ß‡•Ä‡§§‡•Ç‡§´‡§æ‡§® ‡§Æ‡•á‡§Ç ‡§∏‡§¨‡§∏‡•á ‡§ï‡§Æ ‡§™‡•Ä ‡§è‡§ö ‡§®‡§µ‡§Æ‡•ç‡§¨‡§∞ ‡§Æ‡•á‡§Ç ‡§Æ‡§æ‡§™‡§æ ‡§ó‡§Ø‡§æ ‡§•‡§æ‡•§ _END</td>\n",
       "      <td>the lowest ph value recorded for a storm in northeastern united states was during november</td>\n",
       "      <td>15</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                        hindi                                                                                                                                                             english  length_eng_sentence  length_hin_sentence\n",
       "1255665  START_ ‡§Æ‡§π‡§æ‡§∞‡§æ‡§ú‡§æ ‡§ï‡•â‡§≤‡§ø‡§ú ‡§ú‡§Ø‡§™‡•Å‡§∞ _END                                                                                                                                                                       maharaja college jaipur                                                                                                                                             3                    5                  \n",
       "1238476  START_ ‡§á‡§® ‡§ó‡•â‡§° ‡§µ‡•Ä ‡§ü‡•ç‡§∞‡§∏‡•ç‡§ü _END                                                                                                                                                                          in god we trust                                                                                                                                                     4                    6                  \n",
       "1411628  START_ ‡§µ‡§π ‡§®‡•å‡§ï‡§∞‡•Ä ‡§¢‡•Ç‡§Å‡§¢‡§º‡§®‡•á ‡§Æ‡•á‡§Ç ‡§Ö‡§™‡§®‡•Ä ‡§Ö‡§∏‡§´‡§≤‡§§‡§æ ‡§ï‡•Ä ‡§ö‡§∞‡•ç‡§ö‡§æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§Ö‡§Ç‡§§ ‡§Æ‡•á‡§Ç ‡§µ‡§π ‡§® ‡§™‡§æ‡§∏ ‡§® ‡§´‡§º‡•Ä‡§∏ ‡§ï‡•á ‡§¨‡§æ‡§¶ ‡§™‡§∞ ‡§ö‡§≤‡§æ‡§Ø‡•Ä ‡§ú‡§æ ‡§∞‡§π‡•Ä ‡§è‡§ï ‡§ï‡§ï‡•ç‡§∑‡§æ ‡§Æ‡•á‡§Ç ‡§µ‡§ø‡§¶‡•ç‡§Ø‡§æ‡§∞‡•ç‡§•‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•ã ‡§™‡§æ‡§† ‡§∞‡§ü‡§æ ‡§∞‡§π‡§æ ‡§π‡•ã‡§§‡§æ ‡§π‡•à‡•§ _END                                       he recounts his unsuccessful attempts to get a job he is finally left running a crammer s class with the promise no pass no fees                                    25                   35                 \n",
       "1547009  START_ ‡§∞‡§æ‡§ú‡§∂‡§æ‡§π‡•Ä ‡§∏‡§æ‡§Æ‡•ç‡§∞‡§æ‡§ú‡•ç‡§Ø ‡§¨‡§π‡§æ‡§¶‡•Å‡§∞ ‡§∂‡§æ‡§π ‡§¶‡•ç‡§µ‡§ø‡§§‡•Ä‡§Ø ‡§ï‡•á ‡§¨‡§æ‡§¶ ‡§∏‡§Æ‡§æ‡§™‡•ç‡§§ ‡§π‡•ã ‡§ó‡§Ø‡§æ ‡§ú‡•ã ‡§∏‡§ø‡§™‡§æ‡§π‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•Ä ‡§¨‡§ó‡§æ‡§µ‡§§ ‡§Æ‡•á‡§Ç ‡§∏‡§π‡§æ‡§Ø‡§§‡§æ ‡§¶‡•á‡§®‡•á ‡§ï‡•á ‡§∏‡§Ç‡§¶‡•á‡§π ‡§™‡§∞ ‡§¨‡•ç‡§∞‡§ø‡§ü‡§ø‡§∂ ‡§∞‡§æ‡§ú ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§∞‡§Ç‡§ó‡•Ç‡§® ‡§®‡§ø‡§∞‡•ç‡§µ‡§æ‡§∏‡§ø‡§§ ‡§ï‡§∞ ‡§¶‡§ø‡§è ‡§ó‡§è ‡§•‡•á‡•§ ‡§µ‡§π‡§æ‡§Ç ‡§Æ‡•á‡§Ç ‡§â‡§®‡§ï‡•Ä ‡§Æ‡•É‡§§‡•ç‡§Ø‡•Å ‡§π‡•ã ‡§ó‡§à‡•§ _END  the imperial dynasty became extinct with bahadur shah ii who was deported to rangoon by the british on suspicion of assisting the sepoy mutineers he died there in  28                   37                 \n",
       "1344715  START_ ‡§∏‡§Ç‡§Ø‡•Å‡§ï‡•ç‡§§ ‡§∞‡§æ‡§ú‡•ç‡§Ø ‡§Ö‡§Æ‡•á‡§∞‡§ø‡§ï‡§æ ‡§ï‡•á ‡§™‡•Ç‡§∞‡•ç‡§µ‡•ã‡§§‡•ç‡§§‡§∞ ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞ ‡§Æ‡•á‡§Ç ‡§Ü‡§è ‡§Ü‡§Ç‡§ß‡•Ä‡§§‡•Ç‡§´‡§æ‡§® ‡§Æ‡•á‡§Ç ‡§∏‡§¨‡§∏‡•á ‡§ï‡§Æ ‡§™‡•Ä ‡§è‡§ö ‡§®‡§µ‡§Æ‡•ç‡§¨‡§∞ ‡§Æ‡•á‡§Ç ‡§Æ‡§æ‡§™‡§æ ‡§ó‡§Ø‡§æ ‡§•‡§æ‡•§ _END                                                                                    the lowest ph value recorded for a storm in northeastern united states was during november                                                                          15                   21                 "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2528, 4)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['length_eng_sentence']>30].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df['length_eng_sentence']<=20]\n",
    "df=df[df['length_hin_sentence']<=20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17559, 4)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum length of Hindi Sentence  20\n",
      "maximum length of English Sentence  20\n"
     ]
    }
   ],
   "source": [
    "print(\"maximum length of Hindi Sentence \",max(df['length_hin_sentence']))\n",
    "print(\"maximum length of English Sentence \",max(df['length_eng_sentence']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_src=max(df['length_hin_sentence'])\n",
    "max_length_tar=max(df['length_eng_sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28326, 38586)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_words = sorted(list(all_eng_words))\n",
    "target_words = sorted(list(all_hindi_words))\n",
    "num_encoder_tokens = len(all_eng_words)\n",
    "num_decoder_tokens = len(all_hindi_words)\n",
    "num_encoder_tokens, num_decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_decoder_tokens += 1 #for zero padding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])\n",
    "target_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_input_char_index = dict((i, word) for word, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, word) for word, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hindi</th>\n",
       "      <th>english</th>\n",
       "      <th>length_eng_sentence</th>\n",
       "      <th>length_hin_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>759649</th>\n",
       "      <td>START_ ‡§ï‡§∂‡•Ä‡§¶‡§æ _END</td>\n",
       "      <td>fancywork</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571088</th>\n",
       "      <td>START_ ‡§µ‡§®‡§∏‡•ç‡§™‡§§‡§ø _END</td>\n",
       "      <td>forest</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528210</th>\n",
       "      <td>START_ ‡§è‡§¶‡•á‡§ï _END</td>\n",
       "      <td>movement for social democracy</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611869</th>\n",
       "      <td>START_ ‡§ê‡§∏‡§æ ‡§ï‡§π‡§æ ‡§ú‡§æ‡§§‡§æ ‡§π‡•à ‡§ï‡§ø ‡§µ‡•á ‡§ï‡§º‡•Å‡§∞‡§æ‡§® ‡§î‡§∞ ‡§≠‡§ó‡§µ‡§¶‡•ç ‡§ó‡•Ä‡§§‡§æ ‡§¶‡•ã‡§®‡•ã‡§Ç ‡§ï‡§æ ‡§Ö‡§ß‡•ç‡§Ø‡§® ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç‡•§ _END</td>\n",
       "      <td>they say that he studies koran as well as bhagwad gita</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851193</th>\n",
       "      <td>START_ ‡§∂‡§§ _END</td>\n",
       "      <td>c</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633393</th>\n",
       "      <td>START_ ‡§™‡•ç‡§∞‡§¶‡•Ç‡§∑‡§£ ‡§´‡•à‡§≤‡§æ‡§®‡•á ‡§¶‡•á‡§§‡§æ ‡§∏‡§ø‡§¶‡•ç‡§ß‡§æ‡§Ç‡§§ polluter pays principle _END</td>\n",
       "      <td>doctrine on causes of pollution</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969679</th>\n",
       "      <td>START_ ‡§°‡§ø‡§∏‡•ç‡§ï ‡§ï‡•à‡§∂‡§ø‡§Ç‡§ó ‡§Æ‡•á‡§Ç ‡§≤‡§ø‡§ñ‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§°‡§ø‡§∏‡•ç‡§ï ‡§ï‡•à‡§∂‡•á ‡§ï‡§æ ‡§á‡§∏‡•ç‡§§‡•á‡§Æ‡§æ‡§≤ ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ ‡§∞‡§π‡§æ ‡§π‡•à‡•§ _END</td>\n",
       "      <td>disk cache is being used for writing in disk caching</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481117</th>\n",
       "      <td>START_ ‡§∏‡•Ç‡§ö‡•Ä ‡§¶‡•É‡§∂‡•ç‡§Ø ‡§ï‡•á ‡§Ü‡§∞‡§Ç‡§≠ ‡§ï‡§∞‡§®‡•á ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§§‡•ç‡§∞‡•Å‡§ü‡§ø ‡§Ü‡§à _END</td>\n",
       "      <td>the list view encountered an error while starting up</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250572</th>\n",
       "      <td>START_ ‡§™‡§∞‡§ø‡§ö‡§Ø _END</td>\n",
       "      <td>parichay</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1212178</th>\n",
       "      <td>START_ ‡§π‡§Æ‡§æ‡§∞‡•á ‡§∏‡§ï‡§≤ ‡§ò‡§∞‡•á‡§≤‡•Ç ‡§â‡§§‡•ç‡§™‡§æ‡§¶‡§® ‡§Æ‡•á‡§Ç‡§î‡§∞ ‡§Æ‡•á‡§Ç ‡§ï‡•ç‡§∞‡§Æ‡§∂‡§É ‡§î‡§∞‡§™‡•ç‡§∞‡§§‡§ø‡§∂‡§§ ‡§µ‡•É‡§¶‡•ç‡§ß‡§ø ‡§π‡•Å‡§à ‡§π‡•à‡•§ _END</td>\n",
       "      <td>fiscal expansionary measures helped in maintaining the growth momentum amidst downturn anxieties emanating from the global markets</td>\n",
       "      <td>17</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                    hindi                                                                                                                             english  length_eng_sentence  length_hin_sentence\n",
       "759649   START_ ‡§ï‡§∂‡•Ä‡§¶‡§æ _END                                                                 fancywork                                                                                                                           1                    3                  \n",
       "571088   START_ ‡§µ‡§®‡§∏‡•ç‡§™‡§§‡§ø _END                                                               forest                                                                                                                              1                    3                  \n",
       "528210   START_ ‡§è‡§¶‡•á‡§ï _END                                                                  movement for social democracy                                                                                                       4                    3                  \n",
       "611869   START_ ‡§ê‡§∏‡§æ ‡§ï‡§π‡§æ ‡§ú‡§æ‡§§‡§æ ‡§π‡•à ‡§ï‡§ø ‡§µ‡•á ‡§ï‡§º‡•Å‡§∞‡§æ‡§® ‡§î‡§∞ ‡§≠‡§ó‡§µ‡§¶‡•ç ‡§ó‡•Ä‡§§‡§æ ‡§¶‡•ã‡§®‡•ã‡§Ç ‡§ï‡§æ ‡§Ö‡§ß‡•ç‡§Ø‡§® ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç‡•§ _END   they say that he studies koran as well as bhagwad gita                                                                              11                   17                 \n",
       "851193   START_ ‡§∂‡§§ _END                                                                    c                                                                                                                                   1                    3                  \n",
       "633393   START_ ‡§™‡•ç‡§∞‡§¶‡•Ç‡§∑‡§£ ‡§´‡•à‡§≤‡§æ‡§®‡•á ‡§¶‡•á‡§§‡§æ ‡§∏‡§ø‡§¶‡•ç‡§ß‡§æ‡§Ç‡§§ polluter pays principle _END                  doctrine on causes of pollution                                                                                                     5                    9                  \n",
       "969679   START_ ‡§°‡§ø‡§∏‡•ç‡§ï ‡§ï‡•à‡§∂‡§ø‡§Ç‡§ó ‡§Æ‡•á‡§Ç ‡§≤‡§ø‡§ñ‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§°‡§ø‡§∏‡•ç‡§ï ‡§ï‡•à‡§∂‡•á ‡§ï‡§æ ‡§á‡§∏‡•ç‡§§‡•á‡§Æ‡§æ‡§≤ ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ ‡§∞‡§π‡§æ ‡§π‡•à‡•§ _END  disk cache is being used for writing in disk caching                                                                                10                   16                 \n",
       "481117   START_ ‡§∏‡•Ç‡§ö‡•Ä ‡§¶‡•É‡§∂‡•ç‡§Ø ‡§ï‡•á ‡§Ü‡§∞‡§Ç‡§≠ ‡§ï‡§∞‡§®‡•á ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§§‡•ç‡§∞‡•Å‡§ü‡§ø ‡§Ü‡§à _END                              the list view encountered an error while starting up                                                                                9                    11                 \n",
       "1250572  START_ ‡§™‡§∞‡§ø‡§ö‡§Ø _END                                                                 parichay                                                                                                                            1                    3                  \n",
       "1212178  START_ ‡§π‡§Æ‡§æ‡§∞‡•á ‡§∏‡§ï‡§≤ ‡§ò‡§∞‡•á‡§≤‡•Ç ‡§â‡§§‡•ç‡§™‡§æ‡§¶‡§® ‡§Æ‡•á‡§Ç‡§î‡§∞ ‡§Æ‡•á‡§Ç ‡§ï‡•ç‡§∞‡§Æ‡§∂‡§É ‡§î‡§∞‡§™‡•ç‡§∞‡§§‡§ø‡§∂‡§§ ‡§µ‡•É‡§¶‡•ç‡§ß‡§ø ‡§π‡•Å‡§à ‡§π‡•à‡•§ _END     fiscal expansionary measures helped in maintaining the growth momentum amidst downturn anxieties emanating from the global markets  17                   13                 "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = shuffle(df)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14047,), (3512,))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = df['english'], df['hindi']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,random_state=42)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us save this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_pickle('X_train.pkl')\n",
    "X_test.to_pickle('X_test.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(X = X_train, y = y_train, batch_size = 128):\n",
    "    ''' Generate a batch of data '''\n",
    "    while True:\n",
    "        for j in range(0, len(X), batch_size):\n",
    "            encoder_input_data = np.zeros((batch_size, max_length_src),dtype='float32')\n",
    "            decoder_input_data = np.zeros((batch_size, max_length_tar),dtype='float32')\n",
    "            decoder_target_data = np.zeros((batch_size, max_length_tar, num_decoder_tokens),dtype='float32')\n",
    "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
    "                for t, word in enumerate(input_text.split()):\n",
    "                    encoder_input_data[i, t] = input_token_index[word] # encoder input seq\n",
    "                for t, word in enumerate(target_text.split()):\n",
    "                    if t<len(target_text.split())-1:\n",
    "                        decoder_input_data[i, t] = target_token_index[word] # decoder input seq\n",
    "                    if t>0:\n",
    "                        # decoder target sequence (one hot encoded)\n",
    "                        # does not include the START_ token\n",
    "                        # Offset by one timestep\n",
    "                        decoder_target_data[i, t - 1, target_token_index[word]] = 1.\n",
    "            yield([encoder_input_data, decoder_input_data], decoder_target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoder Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb =  Embedding(num_encoder_tokens, latent_dim, mask_zero = True)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(num_decoder_tokens, latent_dim, mask_zero = True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, None, 300)    8497800     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, None, 300)    11576100    ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 300),        721200      ['embedding[0][0]']              \n",
      "                                 (None, 300),                                                     \n",
      "                                 (None, 300)]                                                     \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, None, 300),  721200      ['embedding_1[0][0]',            \n",
      "                                 (None, 300),                     'lstm[0][1]',                   \n",
      "                                 (None, 300)]                     'lstm[0][2]']                   \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, None, 38587)  11614687    ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 33,130,987\n",
      "Trainable params: 33,130,987\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = len(X_train)\n",
    "val_samples = len(X_test)\n",
    "batch_size = 128\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\monas\\AppData\\Local\\Temp\\ipykernel_10376\\2713035456.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(generator = generate_batch(X_train, y_train, batch_size = batch_size),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 92/109 [========================>.....] - ETA: 41s - loss: 2.5473"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'model/embedding/embedding_lookup' defined at (most recent call last):\n    File \"C:\\Users\\monas\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\monas\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\monas\\anaconda3\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"C:\\Users\\monas\\anaconda3\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"C:\\Users\\monas\\anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n      await result\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2863, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2909, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3309, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3369, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\monas\\AppData\\Local\\Temp\\ipykernel_10376\\2713035456.py\", line 1, in <cell line: 1>\n      model.fit_generator(generator = generate_batch(X_train, y_train, batch_size = batch_size),\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 2507, in fit_generator\n      return self.fit(\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\layers\\core\\embedding.py\", line 208, in call\n      out = tf.nn.embedding_lookup(self.embeddings, inputs)\nNode: 'model/embedding/embedding_lookup'\nindices[72,0] = 28326 is not in [0, 28326)\n\t [[{{node model/embedding/embedding_lookup}}]] [Op:__inference_train_function_13999]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[1;32mIn [48]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgenerate_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_samples\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgenerate_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_samples\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:2507\u001b[0m, in \u001b[0;36mModel.fit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2495\u001b[0m \u001b[38;5;124;03m\"\"\"Fits the model on data yielded batch-by-batch by a Python generator.\u001b[39;00m\n\u001b[0;32m   2496\u001b[0m \n\u001b[0;32m   2497\u001b[0m \u001b[38;5;124;03mDEPRECATED:\u001b[39;00m\n\u001b[0;32m   2498\u001b[0m \u001b[38;5;124;03m  `Model.fit` now supports generators, so there is no longer any need to\u001b[39;00m\n\u001b[0;32m   2499\u001b[0m \u001b[38;5;124;03m  use this endpoint.\u001b[39;00m\n\u001b[0;32m   2500\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2501\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   2502\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Model.fit_generator` is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2503\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future version. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2504\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use `Model.fit`, which supports generators.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2505\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   2506\u001b[0m )\n\u001b[1;32m-> 2507\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2509\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2513\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2514\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2515\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2519\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2521\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2522\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'model/embedding/embedding_lookup' defined at (most recent call last):\n    File \"C:\\Users\\monas\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\monas\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\monas\\anaconda3\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"C:\\Users\\monas\\anaconda3\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"C:\\Users\\monas\\anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n      await result\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2863, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2909, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3309, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3369, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\monas\\AppData\\Local\\Temp\\ipykernel_10376\\2713035456.py\", line 1, in <cell line: 1>\n      model.fit_generator(generator = generate_batch(X_train, y_train, batch_size = batch_size),\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 2507, in fit_generator\n      return self.fit(\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\layers\\core\\embedding.py\", line 208, in call\n      out = tf.nn.embedding_lookup(self.embeddings, inputs)\nNode: 'model/embedding/embedding_lookup'\nindices[72,0] = 28326 is not in [0, 28326)\n\t [[{{node model/embedding/embedding_lookup}}]] [Op:__inference_train_function_13999]"
     ]
    }
   ],
   "source": [
    "model.fit_generator(generator = generate_batch(X_train, y_train, batch_size = batch_size),\n",
    "                    steps_per_epoch = train_samples//batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n",
    "                    validation_steps = val_samples//batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 216, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"model\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 1) dtype=string>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [51]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m          \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m          \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_samples\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgenerate_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_samples\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file5rq53mo8.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\monas\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 216, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"model\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 1) dtype=string>]\n"
     ]
    }
   ],
   "source": [
    "model.fit(x = X_train,\n",
    "          y = y_train,\n",
    "          batch_size = batch_size,\n",
    "          steps_per_epoch = train_samples//batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n",
    "          validation_steps = val_samples//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('nmt_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the input sequence to get the \"thought vectors\"\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "dec_emb2= dec_emb_layer(decoder_inputs) # Get the embeddings of the decoder sequence\n",
    "\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2) # A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = target_token_index['START_']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '_END' or\n",
    "           len(decoded_sentence) > 50):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = generate_batch(X_train, y_train, batch_size = 1)\n",
    "k=-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Hindi Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Hindi Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Hindi Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Hindi Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Hindi Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Hindi Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Hindi Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Hindi Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Hindi Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Hindi Translation:', decoded_sentence[:-4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
